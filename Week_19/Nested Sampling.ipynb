{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Nested Sampling\n",
    "\n",
    "Markov chain Monte Carlo (MCMC) methods of parameter estimation (for example) have been hugely successful in many areas where statistical analysis are of vital importance. However, there remain some issues. Some popular MCMC methods, like Hastings-Metropolis or Hamiltonian, run into problems with a multimodal posterior distribution, or one with large (curving) degeneracies between parameters. There is the additional \"tweaking\" of things such as \"step size\" and the long time required to converge to a solution for some problems.\n",
    "\n",
    "Nested sampling (NS) is a concept originally proposed by Skilling (2004) which is targeted at the efficient calculation of the evidence, but also produces posterior inferences (and uncertainties) as a by-product. Graphically, an elliptical bound containing the current point set at each stage of the process is used to restrict the region around the posterior peak from which new samples are drawn. Multiple peaks in the parameter space can be detected and isolated, with separate ellipsoidal bounds constructed around each mode. Computational loads are reduced further by computing a mean and standard uncertainty in one sampling, eliminating the need for multiple runs. Depending upon the specifics of the problem an improvement over approaches like Hamiltonian Monte Carlo is that nested sampling does not require a differentiable phase space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Review\n",
    "Bayesian inference provides a principled approach to determining a set of parameters $\\Theta$ in a model (or hypothesis), $H$, for data, $\\bf{D}$. Bayes theorum states that\n",
    "\n",
    "$$Pr(\\Theta \\vert \\bf{D},\\it{H})=\\frac{Pr(\\bf{D} \\vert \\Theta,\\it{H})Pr(\\bf{\\Theta} \\vert \\it{H})}{Pr(\\bf{D} \\vert \\it{H})},$$\n",
    "\n",
    "where $Pr(\\Theta \\vert \\bf{D},\\it{H}) \\equiv P(\\rm\\Theta \\vert \\bf{D})$ is the posterior probability density of the model parameters, $Pr(\\bf{D} \\vert \\Theta,\\it{H}) \\equiv \\mathcal{L}(\\rm\\Theta)$ the likelihood of the data, and $Pr(\\bf{\\Theta} \\vert \\it{H}) \\equiv \\rm\\pi(\\Theta)$ the parameter prior. The final term, $Pr(\\bf{D} \\vert \\it{H}) \\equiv \\mathcal{Z}$ (the Bayesian evidence), represents the factor required to normalize the posterior over the domain of $\\Theta$ given by:\n",
    "\n",
    "$$ \\mathcal{Z}=\\int_{\\Omega_\\Theta} \\mathcal{L}(\\Theta)\\pi(\\Theta)d(\\Theta).$$\n",
    "\n",
    "Model selection between two competing models, $H_0$ and $H_1$ can be achieved by comparing their respective posterior probabilities given the observed dataset as follows:\n",
    "\n",
    "$$ R=\\frac{Pr(H_1 \\vert \\bf{D})}{Pr(H_0 \\vert \\bf{D})}=\\frac{Pr(\\bf{D} \\vert \\it{H_1})Pr(H_1)}{Pr(\\bf{D} \\vert \\it{H_0})Pr(H_0)}=\\frac{\\mathcal{Z_1}}{\\mathcal{Z_0}}\\frac{Pr(H_1)}{Pr(H_0)}.$$\n",
    "\n",
    "Thus, Bayesian evidence plays a central role in Bayesian model selection.\n",
    "\n",
    "As the average of the likelihood over the prior, the evidence is generally larger for a model if more of its parameter space is likely and smaller for a model with large areas in its parameter space having low likelihood values, even if the likelihood function is shaprly peaked. So, the evidence may be seen both as penalizing 'fine tuning' of a model against the observed data and as an automatic implementation of Occam's Razor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Nested Sampling\n",
    "\n",
    "Nested sampling estimates the Bayesian evidence by transforming the multi-dimensional evidence integral over the prior density into a one-dimensional integral over an inverse survival function (with respect to prior mass) for the likelihood itself. This is accomplished by considering the survival function $X(\\lambda)$, for $\\mathcal{L}(\\Theta)$, dubbed \"the prior volume\" here; namely,\n",
    "\n",
    "$$ X(\\lambda)=\\int_{\\Theta:\\mathcal{L}(\\Theta)>\\lambda}\\pi(\\Theta)d(\\Theta),$$\n",
    "\n",
    "where the integral extends over the region(s) of parameter space contained within the iso-likelihood contour, $\\mathcal{L}(\\Theta)=\\lambda$. Recalling that the expectation value of a non-negative random variable may be recovered by integration over its survival function (a result evident from integration by parts) we have (unconditionally):\n",
    "\n",
    "$$ \\mathcal{Z}=\\int_{0}^{\\infty} X(\\lambda)d\\lambda.$$\n",
    "\n",
    "When $\\mathcal{L}(X)$, the inverse of $X(\\lambda)$, exists (i.e., when $\\mathcal{L}(\\Theta)$ is a continuous function with connected support) the evidence integral may thus be further rearranged as:\n",
    "\n",
    "$$ \\mathcal{Z}=\\int_{0}^{1} \\mathcal{L}(X)dX.$$\n",
    "\n",
    "Now, if $\\mathcal{L}(X)$ is known exactly (and Riemann integrable), be evaluating likelihoods, $\\mathcal{L_i}=\\mathcal{L}(X_i)$, for a deterministic sequence of $X$ values,\n",
    "\n",
    "$$ 0<X_N< \\ldots < X_2 < X_1 < X_0 = 1,$$\n",
    "\n",
    "the evidence could in principle be approximated numerically using only standard quadrature methods as follows:\n",
    "\n",
    "$$ \\mathcal{Z} \\approx \\mathcal{\\hat{Z}}=\\sum_{i=1}^{N} \\mathcal{L_i}\\omega_i,$$\n",
    "\n",
    "where the weights, $\\omega_i$, for the simple trapezium rule are given by $\\omega_i=\\frac{1}{2}(X_{i-1}-X_{i+1})$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Graphical representation\n",
    "\n",
    "Let's take an aside from the discussion heavy in detailed lexicography. What is the basic interpretation of the method of nested sampling from a graphical perspective? What is the process that is implemented?\n",
    "\n",
    "<img src=\"method.jpg\">\n",
    "\n",
    "Closing in on the best value of a parameter looks something like....\n",
    "\n",
    "<img src=\"shrink.jpg\">\n",
    "\n",
    "Even for multiple peaks, say an overlapping point spread function in an image, the algorithm will close in on each separate peak, allowing for estimates of properties of each, no matter the conditions of the surface(s).\n",
    "\n",
    "<img src=\"2D_example.jpg\">\n",
    "\n",
    "What about on multiple peaks or for multiple parameters, maybe even correlated parameters (posteriors) you wish to model? The NS algorithm will separate out each (in the simplest case) while progressing.\n",
    "\n",
    "<img src=\"Multi_mode_example.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Stopping criterion\n",
    "The NS algorithm should terminate when the expected evidence contribution from the current set of live points is less than a user-defined tolerance. This expected remaining contribution can be estimated (cautiously) as $\\Delta\\mathcal{Z_i}=\\mathcal{L_{max}}X_i$, where $\\mathcal{L_{max}}$ is the maximum likelihood value amongst the current set of live points (with $X_i$ the expected value of remaining prior volume, as before)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Posterior inferences\n",
    "\n",
    "Although the NS algorithm is designed specifically to estimate the Bayesian evidence, inferences from the posterior distribution can be easily obtained using the final live points and the full sequence of discarded points from the NS process, i.e., the points with the lowest likelihood value at each iteration of the algorithm. Each such point is simply assigned the importance weight,\n",
    "\n",
    "$$ p_i=\\frac{\\mathcal{L_i}\\omega_i}{\\sum_{j}\\mathcal{L_i}\\omega_i}=\\frac{\\mathcal{L_i}\\omega_i}{\\mathcal{\\hat{Z}}},$$\n",
    "\n",
    "from which sample-based estimates for the key posterior parameter summaries (e.g. means, standard deviations, covariances and so on) may be computed. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### References\n",
    "1. Original \"Nested Sampling\" reference (2004): http://adsabs.harvard.edu/abs/2004AIPC..735..395S\n",
    "2. 2008 MNRAS, 384, 449 http://adsabs.harvard.edu/abs/2008MNRAS.384..449F\n",
    "3. 2009 MNRAS, 398, 2049 http://adsabs.harvard.edu/abs/2009MNRAS.398.2049F\n",
    "4. 2014, only on ArXiv, https://arxiv.org/abs/1306.2144\n",
    "5. PyMultiNest: http://adsabs.harvard.edu/abs/2014A%26A...564A.125B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
