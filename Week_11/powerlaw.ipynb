{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Maximum Likelihood Functions: Workshop (part 2)\n",
    "\n",
    "## Application on Luminosity Functions\n",
    "\n",
    "Studying the dependence of the number of objects in a survey on the luminosity or flux or photon counts is a powerful tool in Astrophysics. One of the most universal and commonly used form of the _luminosity function_ (LF) is the power law:\n",
    "\n",
    "$$N(>L) = K L^{-a}$$\n",
    "\n",
    "where $K$ is a normalization factor and $a$ is the slope.\n",
    "\n",
    "The LFs are actually _luminosity distributions_. Be warned that the above expression __cannot__ describe a probability distribution with support the full range of luminosities: $\\left[0, \\infty\\right)$. It is mathematically necessary to introduce a lower bound: $L_\\min$. We can also use an upper bound (if imposed by the theory or data) but we will not consider this case here.\n",
    "\n",
    "Our distribution's function (CDF) is therefore\n",
    "\n",
    "$$\n",
    "F(L) = \\begin{cases}\n",
    "            1 - \\left(\\frac{L}{L_\\min}\\right)^{-a} & \\text{ for } L \\geq L_\\min \\\\\n",
    "            0 & \\text{ for } L < L_\\min\n",
    "       \\end{cases}\n",
    "$$\n",
    "\n",
    "### Generating a sample\n",
    "\n",
    "The `scipy.stats` package defines a power law distribution defined in the range $\\left[0, \\, 1\\right]$ and for positive exponent --- not the one we are interested it. We can create our own sampler by employing the __inverse transfrom method__ (for more, _Luc Devroye (1998). Non-Uniform Random Variate Generation_: http://www.eirene.de/Devroye.pdf) that depends only on the knowledge of the inverse CDF:\n",
    "\n",
    "> Let $F_X(x)$ be a cumulative distribution function and $Y$ a conntinuous random variable $\\sim \\mathcal{U}\\left[0, 1\\right]$, then $X = F^{-1}\\left(Y\\right)$ is distributed as $F_X$\n",
    "\n",
    "Consequently, if $\\mathbf{X}$ is a sample from standard uniform distribution, then\n",
    "\n",
    "$$\\mathbf{L} = L_\\min \\left(\\mathbf{I} - \\mathbf{X}\\right)^{-\\frac{1}{a}}$$\n",
    "\n",
    "is a power law distributed sample with lower bound.\n",
    "\n",
    "### Explanation of the code\n",
    "\n",
    "The following code demonstrates the above. We have wrote down some help pieces of code:\n",
    "\n",
    "* The class `PowerLawDist` can be used to get the CDF (method `.cdf(x)`) or a sample (method `.rvs(size)`) from the power law distribution with the desired parameters\n",
    "* The function `log_edges(sample, n)` reads finds the minimum (ignoring zeros) and maximum in the sample, and creates `n` values spanning the full range. These values are evenly spaced in log-scale (geometric series). This way we can plot the LF as it is found in the literature: log-scale for both axes.\n",
    "* The function `plot_luminosity_function(sample, bins, **args)` plots the reversed cumulative histogram of the sample defined by the given `bins`. Extra arguments for matplotlib.pyplot.hist (e.g. color) can be added at the end (`args`).\n",
    "\n",
    "In the plot we confirm that the sampling technique and the analytical formula agree."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import scipy.stats as st\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "## Helper class and functions\n",
    "\n",
    "class PowerLawDist:\n",
    "    # this class represents the power law distribution with CDF: 1 - (x / minimum) ^ -slope\n",
    "    def __init__(self, minimum, slope):\n",
    "        # initializes an object with fixed parameters\n",
    "        self.minimum = minimum\n",
    "        self.slope = slope\n",
    "    \n",
    "    def rvs(self, size):\n",
    "        # draws 'size' values from the distribution (using inverse transformation sampling)\n",
    "        return self.minimum * (1.0 - st.uniform.rvs(size = size)) ** (-1.0 / self.slope)\n",
    "    \n",
    "    def cdf(self, x):\n",
    "        # returns the CDF evaluated at 'x' (single number, or numpy array)\n",
    "        return 1.0 - (x / self.minimum) ** (-self.slope)    \n",
    "    \n",
    "def log_edges(data, n):\n",
    "    # returns the geometric series of 'n' terms, to cover the full extend of 'data'\n",
    "    min_in_data = min(data[data > 0.0]) # ignore zeros when getting the minimum value (avoiding -inf)\n",
    "    max_in_data = max(data)\n",
    "    return np.exp(np.linspace(np.log(min_in_data), np.log(max_in_data), n))\n",
    "\n",
    "def plot_luminosity_function(sample, bins, **arguments_for_hist):\n",
    "    # plots a reversed cumulative histogram\n",
    "    plt.hist(sample, bins = bins, cumulative = -1, histtype = \"step\", **arguments_for_hist)\n",
    "    \n",
    "\n",
    "## Sample from power-law and compute the analytical formula\n",
    "Lmin = 10.0\n",
    "a = 1.7\n",
    "N = 1000 # number of sources\n",
    "dist = PowerLawDist(Lmin, a)\n",
    "sample = dist.rvs(size = N)\n",
    "x = log_edges(sample, 30) # compute bin edges\n",
    "y = 1.0 - dist.cdf(x) # analytical P(>L)\n",
    "\n",
    "## Plot\n",
    "plot_luminosity_function(sample, bins = x, normed = True, label = \"empirical\", linewidth = 2)\n",
    "plt.plot(x, y, label = \"from sample\", linewidth = 2) # plot the analytical 1-CDF(x)\n",
    "plt.xscale(\"log\")\n",
    "plt.yscale(\"log\")\n",
    "plt.xlabel(\"counts, $C$\")\n",
    "plt.ylabel(\"$P(>C)$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def P(L, minimum, slope):\n",
    "    ????????\n",
    "\n",
    "def neg_lnL(data, minimum, slope):\n",
    "    ????????\n",
    "\n",
    "trialslopes = np.linspace(a * 0.8, a * 1.2, 30)\n",
    "likelihoods = np.array([neg_lnL(sample, Lmin, slope) for slope in trialslopes])\n",
    "\n",
    "plt.plot(trialslopes, likelihoods, label = r\"$-\\ln{\\mathcal{L}}$\")\n",
    "plt.axvline(a, color = \"k\", label = r\"real value\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Analytical solution?\n",
    "\n",
    "It is possible to derive the analytical solution for this likelihood function too. Though, introducing a more complicated model (Poisson distribution for counts, background, confusion limit, etc.) will create a complicated formula that not only cannot be solved analytically but also can be computationally challenging (numerical overflows when computing factorials, gamma functions etc. / approximations / computational efficiency / multiple minima)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Relation to $\\chi^2$\n",
    "\n",
    "In the least squares method (LSM) we fit the data, e.g. $(x_i, y_i)$ with uncertainties $\\sigma_i$ to a curve (our model) $y = y(x)$. Thus, the parameters of the curve are estimated by minimizing the quantity\n",
    "\n",
    "$$ \\chi^2 = \\sum_i\\left[\\frac{y_i - y(x_i)}{\\sigma_i}\\right]^2 $$\n",
    "\n",
    "__But how this formula is justified?__\n",
    "\n",
    "Assuming Gaussian distribution of the uncertainties, the probability to measure $y_i$ with standard deviation $\\sigma_i$ about the \"actual\" value $y(x_i)$ is\n",
    "\n",
    "$$ P_i = \\frac{1}{\\sigma\\sqrt{2\\pi}} \\exp\\left\\{-\\frac{1}{2}\\left[ \\frac{y_i - y(x_i)}{\\sigma_i} \\right]^2\\right\\} $$\n",
    "\n",
    "Working as before,\n",
    "\n",
    "$$ -\\ln{\\mathcal{L}} = -\\sum_i \\ln P_i = \\sum_i \\ln{\\sigma_i \\sqrt{2\\pi}} + \\frac{1}{2} \\sum_i\\left[\\frac{y_i - y(x_i)}{\\sigma_i}\\right]^2$$\n",
    "\n",
    "Then, minimizing we get:\n",
    "\n",
    "$$ \\frac{\\mathrm{d}}{\\mathrm{d}\\mathcal{L}} \\left(-\\ln \\mathcal{L}\\right)  = 0 $$\n",
    "$$ \\frac{\\mathrm{d}}{\\mathrm{d}\\mathcal{L}} \\sum_i\\left[\\frac{y_i - y(x_i)}{\\sigma_i}\\right]^2 = 0 $$\n",
    "$$ \\frac{\\mathrm{d} \\chi^2}{\\mathrm{d}\\mathcal{L}} = 0 $$\n",
    "\n",
    "So the quantity $\\chi^2$ is defined as the quantity that when minimized, the likelihood is maximized.\n",
    "\n",
    "### In terms of uncertainties...\n",
    "\n",
    "We can estimate the uncertainty of each parameter by defining the region where \n",
    "\n",
    "> $\\ln\\mathcal{L}$ changes by $\\frac{1}{2}$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "> $\\mathcal{L}$ changes by $e^{\\frac{1}{2}}$\n",
    "\n",
    "or equivalently\n",
    "\n",
    "> $\\chi^2$ increases by $1$!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
