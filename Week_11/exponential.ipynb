{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# Likelihood functions: Workshop (part 1)\n",
    "\n",
    "## Previous week's key points\n",
    "\n",
    "* In order to derive a model $M$ _given_ a set of observations $D$ (data), we need to calculate $P(M|D)$ using the __Bayes' Theorem__:\n",
    "\n",
    "$$ P(M|D) = \\frac{P(D|M) P(M)}{P(D)} $$\n",
    "\n",
    "* For now, we simplify the approach by noting that it scales with the _likelihood_:\n",
    "\n",
    "$$ P(M|D) \\sim P(D|M) $$\n",
    "\n",
    "* As the data is a set of observations, $D = \\left\\{x_i\\right\\}$ and the model is described by the parameters $\\left\\{a_i\\right\\}$,\n",
    "\n",
    "$$P(D|M) = P\\left(\\left\\{x_i\\right\\} | \\left\\{a_i\\right\\}\\right)$$\n",
    "\n",
    "* Usually, the observations are independent to each other, so the joint probability is equal to the the product of all probabilities:\n",
    "\n",
    "$$ P(D|M) = P\\left(\\left\\{ x_i \\right\\} | \\left\\{a_i\\right\\}\\right) = \\prod_i P\\left( x_i\\ | \\left\\{a_i\\right\\}\\right) $$\n",
    "\n",
    "* The parameter estimates ___given___ this specific data set, are the ones for which the likelihood is maximized. In practice, what it is easier to __minimize the negated logarithm of the likelihood__ as (i) it produces smaller numbers (avoiding numerical overflows), (ii) allows using sums instead of products and (iii) most of the optimization algorithms are designed to minimize. The problem becomes:\n",
    "\n",
    "$$ \\underset{a_i}{\\arg \\min} \\left(-\\ln{P(D|M)}\\right) \\qquad \\text{or} \\qquad \\underset{a_i}{\\arg \\min} \\left(-\\sum_i\\log{P\\left( x_i\\ | \\left\\{a_i\\right\\}\\right)}\\right)$$\n",
    "\n",
    "## Example A: exponential decay\n",
    "\n",
    "Based on Example 10.1a from __Philip R. Bevington and D. Keith Robinson__ (1992). _Data reduction and error analysis for the physical sciences_. 2nd edition, WCB/McGraw Hill:\n",
    "\n",
    "> Suppose we have a particle detector in our lab, that can trace the trajectories of particles or, equivalently, their lifetime. After an experiment, part of the data is a list of $N$ decay times $t_i$ of $K_s^0$ mesons. __Can we estimate the mean lifetime of the meson?__\n",
    "\n",
    "### Model selection\n",
    "\n",
    "First of all, we should decide which model to use. Namely, __what is the distribution we expect for the data?__. The probability of detecting a particle is\n",
    "\n",
    "$$P_i = A_i \\; p(t_i\\ |\\ \\tau)$$\n",
    "\n",
    "where\n",
    "* $A_i$ is the detection efficiency (e.g. will the particle decay in the volume of our detector?) and\n",
    "* $p(t_i | \\tau)$ is the probability of a particle of mean lifetime $\\tau$ to decay between time $t_i$ and $t_i + \\mathrm{d}t$, so:\n",
    "\n",
    "$$p(t_i\\ |\\ \\tau) \\propto e^{-t_i/\\tau}$$\n",
    "\n",
    "Let's assume that our experimental device can detect all particles (e.g. the travel distance of our particles is small compared to its volume.) Then $A_i = 1$, and we can easily normalize the probability:\n",
    "\n",
    "$$\\int\\limits_{0}^{\\infty} \\ell e^{-t_i/\\tau} \\mathrm{d}t_i = 1 \\Rightarrow \\cdots \\Rightarrow \\ell = \\frac{1}{\\tau}\n",
    "\\Rightarrow P_i = \\frac{1}{\\tau} e^{-t_i / \\tau}\n",
    "$$\n",
    "\n",
    "### Deriving the likelihood function\n",
    "\n",
    "As a shortcut we will use $\\mathcal{L}$ symbol to refer to the likelihood $P(D|M)$. Then,\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "\\mathcal{L} &= \\prod_{i} P_i \\\\\n",
    "-\\ln{\\mathcal{L}} &= -\\sum_{i} \\ln{P_i} \\\\\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "Note that the likelihood depends on the data and $\\tau$ as $P_i = P_i(t_i, \\tau)$.\n",
    "\n",
    "### Maximum likelihood estimation\n",
    "\n",
    "Now, having formulated the likelihood function, we can find the value of $\\tau$ for which $\\mathcal{L}$ is maximized, or equivalently, $-\\ln{\\mathcal{L}}$ is minimized.\n",
    "\n",
    "## Code example\n",
    "\n",
    "In the following script cells we will allow the user to create random samples, write a likelihood function, plot it and maximize it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import scipy.stats as st\n",
    "import scipy.optimize as opt\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "## Simulating the nature\n",
    "real_tau = 12.5\n",
    "real_distribution = st.expon(scale = real_tau)\n",
    "\n",
    "## Simulating the experiment\n",
    "N = 4000\n",
    "our_sample = real_distribution.rvs(size = N)\n",
    "\n",
    "## Plotting histogram of decay times\n",
    "plt.hist(our_sample, 40)\n",
    "plt.xlabel(\"decay time\")\n",
    "plt.ylabel(\"number of decays detected\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "def probability(x, tau):\n",
    "    # returns the probability (or probabilities) of getting 'x'\n",
    "    ????????\n",
    "\n",
    "def neg_log_likelihood(tau, data):\n",
    "    # returns the negated natural logarithm of the likelihood of 'data' given the model parameter 'tau'\n",
    "    ????????\n",
    "\n",
    "# do various evalutations of -logL to plot later\n",
    "taus = np.linspace(real_tau * 0.5, real_tau * 1.5, 30)\n",
    "logLs = [neg_log_likelihood(tau, our_sample) for tau in taus]\n",
    "\n",
    "# use scipy.optimize.minimize to find the minimum\n",
    "opt_tau = opt.minimize_scalar(neg_log_likelihood, args = (our_sample), \\\n",
    "                              method = \"bounded\", bounds = (0, max(our_sample))).x\n",
    "\n",
    "## Output\n",
    "print(\"Real tau =\", real_tau)\n",
    "print(\"Opt. tau =\", opt_tau)\n",
    "plt.plot(taus, logLs, \"b-\", label = r\"$\\ln{\\mathcal{L}}$\")\n",
    "plt.axvline(real_tau, color = \"k\", label = r\"real $\\tau$\")\n",
    "plt.axvline(opt_tau, color = \"r\", label = \"optimization result\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "### Analytical derivation of $\\tau$\n",
    "\n",
    "In the previous example it is actually possible to derive the $\\tau$ that maximizes $L$ analytically:\n",
    "\n",
    "$$\\begin{align}\n",
    "& \\frac{\\mathrm{d}\\mathcal{L}}{\\mathrm{d}{\\tau}} = 0\n",
    "\\Rightarrow \\frac{\\mathrm{d} \\ln\\mathcal{L}}{\\mathrm{d} \\tau} = 0\n",
    "\\Rightarrow \\frac{\\mathrm{d}}{\\mathrm{d} \\tau} \\sum\\limits_{i=1}^{N} \\ln P_i = 0 \\Rightarrow\n",
    "\\frac{\\mathrm{d}}{\\mathrm{d} \\tau} \\sum\\limits_{i=1}^{N} \\ln \\left(\\frac{1}{\\tau} e^{-t_i / \\tau} \\right) = 0 \\Rightarrow \\\\\n",
    "\\Rightarrow & \\frac{\\mathrm{d}}{\\mathrm{d} \\tau} \\sum\\limits_{i=1}^{N} \\left(-\\frac{t_i}{\\tau} - \\ln \\tau\\right) = 0\n",
    "\\Rightarrow \\frac{\\mathrm{d}}{\\mathrm{d} \\tau} \\left( -\\frac{\\sum\\limits_{i=1}^N {t_i}}{\\tau} - N \\ln \\tau\\right) = 0\n",
    "\\Rightarrow \\frac{\\sum\\limits_{i=1}^N {t_i}}{\\tau^2} - \\frac{N}{\\tau} = 0 \\Rightarrow\n",
    "\\\\\n",
    "\\overset{\\tau, N \\neq 0}{\\Rightarrow} & \\tau = \\frac{1}{N}\\sum\\limits_{i=1}^{N} t_i\n",
    "\\end{align}$$\n",
    "\n",
    "> The optimal $\\tau$ is the sample mean of the observations $t_i$ - a great example that the intuitive answer can be justified using probability calculus!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "print(\"The mean decay time in our sample is\", np.mean(our_sample))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## Example B: system failure...\n",
    "\n",
    "> Note: in Bevington's book there are many variations of Example A, illustrating many aspects of the MLE procedure. The following example cannot be found there, as it was created for the SMAC session.\n",
    "\n",
    "Now imagine the following scenario: at the beginning of the experiment, one of the two chips responsible for recording the data, could not register any information in the storage device. After $\\tau_R$ time, the device was repaired and all data could be saved. Because of the time and money required for a rerun of the experiment, it was decided that the best solution is for the incident to be accounted for in the data reduction phase.\n",
    "\n",
    "#### The following code simulates a problematic sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "## Simulate the malfunction\n",
    "repair_time = 10.3 # time at which the malfunction was remedied\n",
    "coinflip = st.bernoulli(0.5) # giving 0 or 1 with probability 0.5 each (coin flip)\n",
    "\n",
    "## Draw a new sample\n",
    "new_sample = []\n",
    "for i in range(N):\n",
    "    t = real_distribution.rvs()\n",
    "    if t < repair_time and coinflip.rvs() == 0: continue\n",
    "    new_sample.append(t)\n",
    "new_sample = np.array(new_sample)\n",
    "print(\"Lost\", N - len(new_sample), \"particles out of\", N)\n",
    "\n",
    "## Histogram\n",
    "plt.hist(new_sample, 40)\n",
    "plt.xlabel(\"decay time\")\n",
    "plt.ylabel(\"number of decays detected\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "#### We will now use the likelihood function we used before..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Maximum Likelihood Estimation\n",
    "taus = np.linspace(real_tau * 0.5, real_tau * 1.5, 30)\n",
    "logLs = [neg_log_likelihood(tau, new_sample) for tau in taus]\n",
    "opt_tau = opt.minimize_scalar(neg_log_likelihood, args = (new_sample), \\\n",
    "                              method = \"bounded\", bounds = (0, max(new_sample))).x\n",
    "\n",
    "## Output\n",
    "plt.plot(taus, logLs, \"k-\", label = r\"$-\\log{\\mathcal{L}}$\")\n",
    "print(\"Opt. tau using old likelihood function:\", opt_tau)\n",
    "plt.axvline(real_tau, color = \"b\", label = r\"real $\\tau$\")\n",
    "plt.axvline(opt_tau, color = \"r\", label = \"optimization result\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "While the optimization algorithm agrees with the plot of $-\\ln{\\mathcal{L}}$, the result apparently is higher than the _real_ one... Why?\n",
    "\n",
    "### Explanation\n",
    "\n",
    "As we proved analytically, the previous likelihood function is minimized by the sample mean of the observations. Due to the malfunction we lost a siginificant number of decays with small values. Consequently, the sample mean __overestimates__ the mean lifetime.\n",
    "\n",
    "## Accounting for the malfunction in the likelihood function\n",
    "\n",
    "#### What changes in the model?\n",
    "\n",
    "Before the repair, we lost half of our data. This means that the detection efficiency is no longer $1$ but depends on time:\n",
    "\n",
    "$$ A(t) = \\begin{cases} 0.5 \\quad &\\text{ for } t < t_R \\\\ 1 \\quad &\\text{ for } t \\geq t_R \\end{cases} $$\n",
    "\n",
    "#### A new probability for each event\n",
    "\n",
    "Now having in mind that the exponential decay law is unaffected by the malfucntion, the _updated_ probability of getting an event with decay time $t$ is:\n",
    "\n",
    "$$ P_\\text{new}(t) = \\ell A(t) e^{-t/\\tau} $$\n",
    "\n",
    "We normalize,\n",
    "$$\n",
    "\\begin{align}\n",
    "&\\int\\limits_{0}^{\\infty} {\\ell A(t) e^{-t/\\tau} \\ \\mathrm{d}t} = 1 \\Rightarrow\n",
    "\\int\\limits_0^{\\tau_R} \\frac{1}{2} e^{-t/\\tau} \\ \\mathrm{d}t + \\int\\limits_{\\tau_R}^\\infty e^{-t/\\tau} \\ \\mathrm{d}t = \\frac{1}{\\ell}\n",
    "\\Rightarrow \\cdots \\Rightarrow \\\\ \\Rightarrow &\\ell = \\frac{2}{\\tau\\left(1 + e^{-\\tau_R/\\tau}\\right)}\n",
    "\\Rightarrow P_\\text{new}(t) = \\frac{2 A(t) e^{-t/\\tau}}{\\tau\\left(1 + e^{\\tau_R/\\tau}\\right)}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "#### New likelihood function\n",
    "\n",
    "We simply use the new event probability:\n",
    "$$ -\\ln{\\mathcal{L}} = -\\sum\\limits_{i=1}^N \\ln{P_\\text{new}(t_i)} $$\n",
    "\n",
    "#### Now, let's try to find the mean lifetime, assuming we know the real repair time..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "## Maximum Likelihood Estimation\n",
    "\n",
    "def new_probability(x, tau):\n",
    "    # returns the probability (or probabilities) of getting 'x'\n",
    "    ????????\n",
    "\n",
    "def new_neg_log_likelihood(tau, data):\n",
    "    # returns the negated natural logarithm of the likelihood of 'data' given the model parameter 'tau'\n",
    "    ????????\n",
    "\n",
    "taus = np.linspace(real_tau / 2.0, real_tau * 1.5, 30)\n",
    "logLs = [new_neg_log_likelihood(tau, new_sample) for tau in taus]\n",
    "opt_tau = opt.minimize_scalar(new_neg_log_likelihood, args = (new_sample), \\\n",
    "                              method = \"bounded\", bounds = (0, max(new_sample))).x\n",
    "\n",
    "# Output\n",
    "print(\"Real tau =\", real_tau)\n",
    "print(\"Opt. tau =\", opt_tau)\n",
    "plt.plot(taus, logLs, \"k-\", label = r\"$-\\ln{\\mathcal{L}}$\")\n",
    "plt.axvline(real_tau, color = \"b\", label = r\"real $\\tau$\")\n",
    "plt.axvline(opt_tau, color = \"r\", label = \"optimization result\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## A 2D likelihood function\n",
    "\n",
    "Suppose that the exact time of the repair could not be recorded (e.g. complicated to analyse the response of the electronics). In the histogram of the sample we can distiguish the bump in the detections, but as that depends on the binning of the histogram we should use a more \"robust\" method: minimizing both $\\tau$ and $\\tau_R$!\n",
    "\n",
    "#### Let's try again, but searching for the mean lifetime and the repair time simulteneously!..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "def new_probability(x, tau, repair_tau):\n",
    "    # returns the probability (or probabilities) of getting 'x'\n",
    "    prob = np.exp(-x / tau) / (tau * (1.0 + np.exp(-repair_tau / tau)))\n",
    "    for i in range(len(x)):\n",
    "        if x[i] < repair_tau: prob[i] /= 2.0\n",
    "    return prob\n",
    "\n",
    "def new_neg_log_likelihood(params, data):\n",
    "    # returns the negated natural logarithm of the likelihood of 'data' given the model parameters [tau, repair_tau]\n",
    "    return -np.sum(np.log(new_probability(data, params[0], params[1])))\n",
    "\n",
    "taus = np.linspace(real_tau * 0.8, real_tau * 1.2, 40)\n",
    "reps = np.linspace(repair_time * 0.8, repair_time * 1.2, 40)\n",
    "\n",
    "Z = np.array([[new_neg_log_likelihood([x, y], new_sample) for x in taus] for y in reps])\n",
    "Z = Z - np.max(Z)\n",
    "opt_tau = opt.minimize_scalar(new_neg_log_likelihood, args = (new_sample), \\\n",
    "                              method = \"bounded\", bounds = (0, max(new_sample))).x\n",
    "\n",
    "print(\"Real parameters =\", [real_tau, repair_time])\n",
    "print(\"Opt. parameters =\", opt_result)\n",
    "plt.contourf(taus, reps, Z, cmap = \"rainbow\")\n",
    "plt.axvline(real_tau, color = \"k\", linestyle = \"-\", label = \"real values\")\n",
    "plt.axhline(repair_time, color = \"k\", linestyle = \"-\")\n",
    "plt.axvline(opt_result[0], color = \"k\", linestyle = \"--\", label = \"optimization\")\n",
    "plt.axhline(opt_result[1], color = \"k\", linestyle = \"--\")\n",
    "\n",
    "plt.xlabel(r\"$\\tau$\")\n",
    "plt.ylabel(r\"$\\tau_R$\")\n",
    "cb = plt.colorbar()\n",
    "cb.set_label(r\"$\\Delta\\left[-\\ln \\mathcal{L}\\right]$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
