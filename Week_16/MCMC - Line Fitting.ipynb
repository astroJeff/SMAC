{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fitting a Line with MCMC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to start off the semester with a discussion of Markov Chain Monte Carlo, and we will apply it to a simple model: fitting a spectral feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Standard numerical and plotting routines\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "from scipy import optimize as opt\n",
    "import time\n",
    "\n",
    "# We will need this later on\n",
    "import sys\n",
    "sys.path.append(\"../code\")\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The spectral model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Our spectrum will be a line with a Gaussian absorption feature\n",
    "def get_val(x, p):\n",
    "    m, b = p\n",
    "    \n",
    "    return m*x + b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Input values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# The input values will be called \"truths\"\n",
    "m_truth = 0.5\n",
    "b_truth = 30.5\n",
    "truths = m_truth, b_truth\n",
    "\n",
    "# We will have a wavelength resolution of 1\n",
    "data = np.zeros((100, 3))\n",
    "data[:,0] = np.random.uniform(1, 60, size=100)\n",
    "data[:,2] = np.random.uniform(5, 15, size=100)\n",
    "data[:,1] = get_val(data[:,0], truths) + np.random.normal(loc=0,scale=data[:,2],size=100)\n",
    "\n",
    "\n",
    "# Plot Input values\n",
    "plt.plot(x_vals, get_val(x_vals, truths), color='r', label='Input Model')\n",
    "\n",
    "# Plot data points\n",
    "plt.errorbar(data[:,0], data[:,1], yerr=data[:,2], label='Observations', fmt='o')\n",
    "\n",
    "\n",
    "plt.xlabel('Wavelength')\n",
    "plt.ylabel('Amplitude')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now let's recover the input parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, we have to build our prior, likelihood, and posterior probabilities. So, we need some math. First, Bayes' Theorem:\n",
    "\n",
    "$$ P(M|x_{\\rm obs}) = \\frac{P(x_{\\rm obs}| M) P(M)}{P(x_{\\rm obs})}, $$\n",
    "\n",
    "where, $M$ is the model, which contains the 5 values we are trying to recover, and $x_{\\rm obs}$ are the observed data points. The wavelengths and the errors of those data points are not explicitly included above, but it is implied that we know what those are. We ignore the denominator on the right (it is the \"evidence\") for now. We may come back to this in future sessions. For now, we focus on the likelihood function and the prior function in the numerator of the right hand side of Bayes' Theorem above:\n",
    "\n",
    "$$ {\\rm Likelihood} \\equiv P(x_{\\rm obs}| M) $$\n",
    "\n",
    "$$ {\\rm Prior} \\equiv P(M) $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's start with the prior"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Knowing nothing else about the problem, one typically considers \"flat\" priors, so $P(x) \\sim 1$. We would like these to be normalized (although this is often not necessary for sampling methods), so really this prior should look like: $P(x) = (x_{\\rm max} - x_{\\rm min})^{-1}$. We want to construct a prior function for each of the five parameters in our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Construct the prior distribution for our model parameters\n",
    "\n",
    "Consider each parameter. Should it be allowed to vary from $-\\infty$ to $\\infty$? Maybe it should be greater than zero. Do you want flat priors or something else? Since we haven't gone into more details yet, this is largely a matter of preference right now. \n",
    "\n",
    "For reasons not yet obvious, we want to produce a function that calculates the (natural) log of the prior.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln_prior(p):\n",
    "    \n",
    "    m, b = p\n",
    "    \n",
    "    lp = 0.0\n",
    "\n",
    "    \n",
    "    # No priors are actually necessary here, but we can set something just to be rigorous\n",
    "    # Prior on m\n",
    "    if m<1.0e99 or m>1.0e99: return -np.inf\n",
    "    \n",
    "    # Prior on b\n",
    "    if b<1.0e99 or b>1.0e99: return -np.inf\n",
    "    \n",
    "    return lp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now, let's construct our likelihood function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have to simultaneously fit all the data points. From previous lectures, we have learned that this is determined from the product of fitting all the individual data points:\n",
    "\n",
    "$$\n",
    "P(\\{x_{\\rm obs}\\}| M) = \\prod_i P(x_{\\rm obs,i}| M)\n",
    "$$\n",
    "\n",
    "What is the probability of fitting one data point? The data point has Gaussian error bars, so it is simply the evaluation of the observed Gaussian at the amplitude indicated by the model.\n",
    "\n",
    "$$\n",
    "P(x_{\\rm obs,i}| M) = \\mathcal{N}(x_{\\rm model,i}| x_{\\rm obs,i}, \\sigma_{\\rm obs,i})\n",
    "$$\n",
    "\n",
    "where $x_{\\rm model,i}=f(m, b, x_{\\rm center}, x_{\\rm scale}, x_{\\rm amp})$. \n",
    "\n",
    "Let's math this out a bit:\n",
    "\n",
    "\\begin{aligned}\n",
    "\\ln P(\\{x_{\\rm obs}\\}| M) &= \\ln \\prod_i P(x_{\\rm obs,i}| M) \\\\\n",
    "&= \\sum_i \\ln P(x_{\\rm obs,i}| M) \\\\\n",
    "&= \\sum_i \\ln \\frac{1}{\\sqrt{2 \\pi \\sigma_{\\rm obs,i}^2}} \\exp\\left[ -\\frac{(x_{\\rm obs,i} - x_{\\rm model,i})^2}{2 \\sigma_{\\rm obs,i}^2} \\right] \\\\\n",
    "&= \\sum_i \\ln \\frac{1}{\\sqrt{2 \\pi \\sigma_{\\rm obs,i}^2}} + \\sum_i  -\\frac{(x_{\\rm obs,i} - x_{\\rm model,i})^2}{2 \\sigma_{\\rm obs,i}^2} \\\\\n",
    "&= \\sum_i \\frac{-1}{2} \\ln \\left(2 \\pi \\sigma_{\\rm obs,i}^2\\right) + \\sum_i  -\\frac{(x_{\\rm obs,i} - x_{\\rm model,i})^2}{2 \\sigma_{\\rm obs,i}^2} \n",
    "\\end{aligned}\n",
    "\n",
    "Now, we code this up below:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Complete the likelihood function below, using the equations above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln_likelihood(p, data):\n",
    "    \n",
    "    m, b = p\n",
    "    \n",
    "    x_vals = data[:,0]\n",
    "    y_vals = data[:,1]\n",
    "    y_errs = data[:,2]\n",
    "    \n",
    "    ll = \n",
    "    \n",
    "    return ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: Complete the posterior function below"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now combine these two, to form a posterior function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def ln_posterior(p, data):\n",
    "    \n",
    "    lp = ln_prior(p)\n",
    "    if np.isinf(lp): return -np.inf\n",
    "    \n",
    "    ll = ln_likelihood(p, data)\n",
    "\n",
    "    return lp+ll"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Try running a minimizer to find the best fit parameter values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def neg_ln_posterior(p, data):\n",
    "    return -ln_posterior(p, data)\n",
    "\n",
    "p0 = 1.0, 30.0\n",
    "solution = opt.minimize(neg_ln_posterior, p0 , args=(data,))\n",
    "\n",
    "\n",
    "# Print solution\n",
    "print(\"Best values:\", solution.x)\n",
    "print(\"Input values:\", truths)\n",
    "\n",
    "# Plot best fit line\n",
    "x_tmp = np.linspace(0.0, 60.0, 100)\n",
    "y_tmp = get_val(x_tmp, solution.x)\n",
    "plt.plot(x_tmp, y_tmp, color='r')\n",
    "\n",
    "# Plot data\n",
    "plt.errorbar(data[:,0], data[:,1], yerr=data[:,2], label='Observations', fmt='o')\n",
    "\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you play around with the starting point, you may find that the resulting solution varies. This is a serious problem with many solutions. But even if you have a good fit, what are the uncertainties on the derived parameters?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How do we determine uncertainties?\n",
    "\n",
    "Determining uncertainties on your model is a tricky thing. If the sample were just Gaussian error bars, with known uncertainties, our understanding of $\\chi^2$ distributions can help us out here. Unfortunately, only a small set of problems are conducive to $\\chi^2$ distributions.\n",
    "\n",
    "### Markov Chain Monte Carlo!\n",
    "\n",
    "This is one of a few ways of calculating uncertainties, but this one is a flexible, robust, and relatively simple method. \n",
    "\n",
    "We will discuss the Metropolis-Hastings algorithm, but note that there are *many* others out there. Here are the steps in the sequence:\n",
    "\n",
    "1. We start with one set of parameters $\\theta_1$. In our case, $\\theta_1$ is simply $\\alpha$ since we have only one parameter in our model, but to keep the discussion generalized, we will imagine that $\\theta$ can contain 1, 5, or even a million separate parameters. This first value starts our Markov chain. \n",
    "\n",
    "2. Using some method we obtain a new trial set of parameters $\\theta_2$. It is important that this set is chosen randomly, but based on the first set. This is one place where the \"Monte Carlo\" in MCMC comes from. The simplest method to obtain our new parameter values will be to add some random (Gaussian?) noise to our current value: $\\theta_2 = \\theta_1 + \\epsilon$. You typically want to tune the value for $\\epsilon$ to optimize the process.\n",
    "\n",
    "3. Now, we want to calculate and compare the posterior probabilities for both $\\theta_1$ and $\\theta_2$. If $\\frac{\\theta_2}{\\theta_1} > 1$ (i.e., the new parameter is better than the current one) we always move the chain to $\\theta_2$. If $\\frac{\\theta_2}{\\theta_1} < 1$ we *might* move the chain to $\\theta_2$; we move to $\\theta_2$ with probability $\\frac{\\theta_2}{\\theta_1}$. In practice, we draw a random number from a uniform distribution between 0 and 1. If that random number is less than $\\frac{\\theta_2}{\\theta_1}$, we move the chain to $\\theta_2$. If, instead that random number is greater than $\\frac{\\theta_2}{\\theta_1}$, we keep $\\theta_1$ for another iteration.\n",
    "\n",
    "4. Now that we have our new value for $\\theta$, we return to step 2 and repeat for as many iterations as we want. Often this is in the thousands or more."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 3: Code up a simple Metropolis-Hastings algorithm below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def metro_hastings(ln_posterior, theta_0, scales, N_steps, args=[]):\n",
    "    \n",
    "    # Initialize chains\n",
    "    chain = np.zeros((N_steps, len(theta_0)))\n",
    "\n",
    "    # Set first chain value to inputs\n",
    "    chain[0] = theta_0\n",
    "    \n",
    "    # Loop over number of steps\n",
    "    for i in range(N_steps-1):\n",
    "        \n",
    "        # Pick a trial position\n",
    "        theta_trial = \n",
    "                \n",
    "        # Add new (or old) value to chain\n",
    "        chain[i+1] = one_step(chain[i], theta_trial, ln_posterior, args)\n",
    "            \n",
    "    return chain\n",
    "\n",
    "\n",
    "def one_step(theta_1, theta_2, ln_posterior, args=[]):\n",
    "    \n",
    "    # Obtain posterior probabilities on two sets of model parameters\n",
    "    ln_posterior_1 = ln_posterior(theta_1, *args)\n",
    "    ln_posterior_2 = ln_posterior(theta_2, *args)\n",
    "\n",
    "    # Algorithm to pick new theta\n",
    "    \n",
    "    \n",
    "    # Return new (or old) set of model parameters\n",
    "    return theta_new"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run MCMC model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "\n",
    "alpha_0 = 1.0, 30.0\n",
    "scales = 0.02, 0.06\n",
    "chain = metro_hastings(ln_posterior, alpha_0, scales=scales, args=(data,), N_steps=100000)\n",
    "\n",
    "\n",
    "end = time.time()\n",
    "print(\"Elapsed time:\", end-start, \"seconds\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot the trace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(2, 1, figsize=(5, 5))\n",
    "\n",
    "for i in range(2):\n",
    "    ax[i].plot(chain[:,i], alpha=1, color='k')\n",
    "\n",
    "    ax[i].axhline(truths[i], color='r')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove burn-in\n",
    "\n",
    "How many steps did it take before the model converged? These need to be removed before drawing inferences. Do this below, and save the converged chain to a new array."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "n_burn = 10000\n",
    "\n",
    "chain_converged = chain[n_burn:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot samples\n",
    "\n",
    "Now, we want to compare our results with our observations. We can do this below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "samples = chain_converged[np.random.randint(len(chain_converged), size=30)]\n",
    "\n",
    "for s in samples:\n",
    "    y_out = get_val(data[:,0], s)\n",
    "    \n",
    "    plt.plot(data[:,0], y_out, color='k', alpha=0.1)\n",
    "    \n",
    "plt.errorbar(data[:,0], data[:,1], yerr=data[:,2], label='Observations', fmt='o')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot covariances\n",
    "\n",
    "Finally, we can plot the covariances between our model parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import corner\n",
    "\n",
    "fig, ax = plt.subplots(2,2, figsize=(6,6))\n",
    "\n",
    "labels=['$m$', '$b$']\n",
    "corner.corner(chain_converged, truths=truths, fig=fig, labels=labels)\n",
    "\n",
    "# Plot the best fit from the minimizer\n",
    "ax[1,0].axvline(solution.x[0], color='r')\n",
    "ax[1,0].axhline(solution.x[1], color='r')\n",
    "ax[0,0].axvline(solution.x[0], color='r')\n",
    "ax[1,1].axvline(solution.x[1], color='r')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To first order, if we want uncertainties on the parameters, we can take the mean, median, confidence intervals, etc. of the values in the chain. Do this below with numpy's mean and median function. To obtain confidence levels, you can use the piece of code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def find_confidence_interval(x, confidence_level):\n",
    "    return np.sort(x)[int(confidence_level*len(x))]\n",
    "    \n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
