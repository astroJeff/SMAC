{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hamiltonian Monte Carlo\n",
    "### (Or: watch your \"p's\"s and \"q's\")\n",
    "\n",
    "Hamiltonian physics is a re-imagining (of sorts) of the fundamental idea of the conservation of energy. \n",
    "\n",
    "The classical formulation goes something like the following. Assume we have a particle, whose postion is denoted by the variable $\\bf{q}$. The momentum of such a particle is defined by the fomula $\\bf{p=mv}$, where $\\bf{v}$ is the first derivative of the position variable, $\\bf{\\dot{q}}$.\n",
    "\n",
    "The kinetic and potential energy of this particle may be represented as:\n",
    "\n",
    "$$\\bf{K(p,q) = \\frac{1}{2}m\\dot{q}^2 \\hspace{1in} U(q)} $$\n",
    "\n",
    "With the $Hamiltonian$ written as the sum of the kinetic and potential energies (scalable to many particles, if needed):\n",
    "\n",
    "$$\\bf{H = K(p,q) + U(q) = \\frac{1}{2}m\\dot{q}^2 + U(q) }$$\n",
    "\n",
    "or\n",
    "\n",
    "$$\\bf{H = \\frac{p}{2m} + U(q)}$$\n",
    "\n",
    "The equation of motion for the particle (or system, for many particles) are given by $Hamilton's~ Equations$:\n",
    "\n",
    "$$\\boxed{\\bf{q=\\frac{\\partial H}{\\partial p}}} \\hspace{1in} \\boxed{\\bf{p=-\\frac{\\partial H}{\\partial q}}}$$\n",
    "\n",
    "This is all fine and dandy, but really? $Why~do~we~care$?\n",
    "\n",
    "Keep this in mind as we move forward: the solution of Hamilton's equations yields a trajectory $-$ positions and momenta as functions of time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some (re-)definitions\n",
    "\n",
    "For purposes here, Markov chain Monte Carlo (MCMC) is a method to determine expectations (some value of interest) from the posterior distribution of our model. To avoid a [traxoline](https://people.physics.tamu.edu/krisciunas/Traxoline.pdf) moment, the best explanation for this that I've found is: the posterior distribution is a probability distribution that represents your updated beliefs about the parameter after having seen the data. From this probability distribution, we can estimate the value of interest, as well as uncertainties in said value. \n",
    "\n",
    "We need the MCMC to converge to the true expectation value (true estimate) quickly. Fast convergence requires strong conditions of $\\bf{ergodicity}$ - that is, a parameter space may be sufficiently explored statistically by MCMC in a finite amount of time. Specifically, the condition of geometric ergodicity is desirable. In this condition, MCMC estimators follow the central limit theorum, and the properly normalized sum of the probability distribution or posterior, tends towards a normal, or Gaussian distribution. $\\bf{Geometric~ergodicity}$ applies to manifolds (high-dimentional surfaces) and has been historically important in the development of differential geometric analyses, such as General Relativity. \n",
    "\n",
    "$Hamiltonian~Monte~Carlo~(\\bf{HMC})$ is unique in that when it fails to converge, it is recognisable. For example, with the split $\\hat{\\bf{R}}$ statistic, which for well-behaved parameter spaces, should be very near 1.0, and values above 1.1 indicate problems with the fit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hamiltonian Monte Carlo application\n",
    "\n",
    "Using Hamiltonian dynamics to sample from a distribution requires translating the density function for this distribution to a potential energy function and introducing \"momentum\" variables to go with the original variables of interest (now seen as \"position\" variables). We can then simulate a Markov chain in which each iteration resamples the momentum and then does a Metropolis update with a proposal found using Hamiltonian dynamics.\n",
    "\n",
    "The first step of the HMC process changes only the momentum, with new values randomly extracted from a Gaussian distribution. In the second step, a Metropolis update is performed, using Hamiltonian dynamics to propose a new state. Care must be taken in choosing the number of steps and step size to avoid problems, such as periodicity in parameter space exploration."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why HMC?\n",
    "\n",
    "Let's explore an example, at least graphically, why one might wish to use HMC over other types of MCMC estimation. This example will contrast HMC and random-walk Metropolis MCMC via a 100-dimensional multivariate Gaussian distribution in which the varialbes are independant, with means of zero, and standard deviations of 0.01, 0.02, ..., 0.99, 1.00. The results of the simulations are best seen in a series of plots.\n",
    "\n",
    "<img src=\"Location_plot.jpg\">\n",
    "<img src=\"Accuracy.jpg\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References:\n",
    "\n",
    "1. http://www.mcmchandbook.net/HandbookChapter5.pdf\n",
    "2. http://stats.stackexchange.com/questions/58564/help-me-understand-bayesian-prior-and-posterior-distributions\n",
    "3. http://www.nyu.edu/classes/tuckerman/stat.mech/lectures/lecture_1/node4.html\n",
    "4. http://mc-stan.org/users/documentation/case-studies/pystan_workflow.html\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
